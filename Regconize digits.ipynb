{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from skimage.feature import hog\n",
    "from PIL import Image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "import keras\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OCR_scan(img_path = '', pre = 'thresh', save_path = '', number_pics = ''):\n",
    "    image = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    if pre == 'thresh':\n",
    "        ret, gray = cv2.threshold(gray, 0, 255,cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "    elif pre =='blur':\n",
    "        gray = cv2.medianBlur(gray, 3)\n",
    "    save_into = save_path+number_pics+'.jpg'\n",
    "    cv2.imwrite(save_into, gray) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = 'crop_image\\\\'\n",
    "pre1 = 'thresh'\n",
    "pre2 = 'blur'\n",
    "save_path1 = 'preprocess_thresh\\\\'\n",
    "save_path2 = 'preprocess_blur\\\\'\n",
    "count = '1'\n",
    "count_str = 1\n",
    "for i in os.listdir(img_list):\n",
    "    img_path = img_list+i\n",
    "    OCR_scan(img_path = img_path, pre = pre1, save_path = save_path1, number_pics = count)\n",
    "    OCR_scan(img_path = img_path, pre = pre2, save_path = save_path2, number_pics = count)\n",
    "    count_str += 1\n",
    "    count = str(count_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'preprocess_thresh\\\\'\n",
    "list_thresh = []\n",
    "list_blur = []\n",
    "list_o = []\n",
    "list_test = []\n",
    "for i in range(28):\n",
    "    name = i+1\n",
    "    thresh = filename + str(name) + '.jpg'\n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    image = cv2.imread(thresh)\n",
    "    erosion = cv2.erode(image,kernel,iterations = 1)\n",
    "#     blur = 'preprocess_blur\\\\'+ str(name) + '.jpg'\n",
    "#     original = 'crop_image\\\\'+ str(name) + '.jpg'\n",
    "#     test = 'preprocess_test\\\\'+ str(name) + '.jpg'\n",
    "    list_thresh.append(pytesseract.image_to_string(erosion))\n",
    "#     list_blur.append(pytesseract.image_to_string(Image.open(blur)))\n",
    "#     list_o.append(pytesseract.image_to_string(Image.open(original)))\n",
    "#     list_test.append(pytesseract.image_to_string(Image.open(test)))\n",
    "\n",
    "# print(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of thresholding is: 59 7\n",
      "664.80\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_path = 'preprocess_blur\\\\1.jpg'\n",
    "img_path = 'preprocess_thresh\\\\1.jpg'\n",
    "\n",
    "# img_path = 'crop_image\\\\1.jpg'\n",
    "# img_path = 'test_license.jpg'\n",
    "# img_path = 'something.jpg'\n",
    "# img_path = '.\\\\training-samples\\\\detection\\\\positives\\\\0_036_000.png'\n",
    "# img_path = 'test_tesseract1.png'\n",
    "# image = Image.open(img_path)\n",
    "image = cv2.imread(img_path,0)\n",
    "# ret, img = cv2.threshold(gray, 0, 255,cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)\n",
    "kernel = np.ones((3,3),np.uint8)\n",
    "erosion = cv2.erode(image,kernel,iterations = 1)\n",
    "opening = cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)\n",
    "closing = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "# plt.subplot(1,2,1,title = 'original'),plt.imshow(img, cmap = 'gray')\n",
    "# plt.subplot(1,2,2,title = 'result'),plt.imshow(erosion, cmap = 'gray')\n",
    "plt.imshow(erosion,cmap = 'gray')\n",
    "config = ('-l eng --oem 1 --psm 3')\n",
    "a = (pytesseract.image_to_string(erosion,config = config))\n",
    "print('Result of thresholding is: {}'.format(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<matplotlib.axes._subplots.AxesSubplot at 0x2d1392b0d30>,\n",
       " <matplotlib.image.AxesImage at 0x2d1393574a8>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = cv2.imread('j.png',0)\n",
    "kernel = np.ones((5,5),np.uint8)\n",
    "erosion = cv2.erode(img,kernel,iterations = 1)\n",
    "plt.subplot(1,2,1,title = 'original'),plt.imshow(img, cmap = 'gray')\n",
    "plt.subplot(1,2,2,title = 'result'),plt.imshow(erosion, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#59 7\n",
      "664.80\n",
      "#62 HS\n",
      "7108\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#uu30e8\n",
      "#59 ET\n",
      "70850\n",
      "#a 59-1\n",
      "902.08\n",
      "#59-81\n",
      "38559\n",
      "#\n",
      "#\n",
      "#3-5]\n",
      "09841\n",
      "#81-61\n",
      "135.56\n",
      "#60-F1\n",
      "506.70)\n",
      "#bz ri]\n",
      "0200\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#60 BI\n",
      "CUE\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#7-1\n",
      "789 10\n",
      "#\n"
     ]
    }
   ],
   "source": [
    "for i in list_thresh:\n",
    "    print('#{}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#59897\n",
      "#B 99°C2\n",
      "012.08\n",
      "#ie\n",
      "043.09\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#39-51\n",
      "098.41\n",
      "#81-61\n",
      "135.56\n",
      "#60:F1\n",
      "506.70\n",
      "#\n",
      "#1\n",
      "0\n",
      "\n",
      "6-H\n",
      "61\n",
      "\n",
      "i\n",
      "#\n",
      "#Gar) ¢\n",
      "7840)\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#136.29\n",
      "#70-01\n",
      "/39 16\n",
      "#39-(2\n",
      "316.01\n"
     ]
    }
   ],
   "source": [
    "for i in list_blur:\n",
    "    print('#{}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#, 59-P1 '\n",
      "664.80\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#136.29\n",
      "#\n",
      "#59-(7\n",
      "316.01\n",
      "#\n",
      "#\n",
      "#\n",
      "#U1]\n",
      "98.97\n",
      "#\n",
      "#78-G)\n",
      "043.08)\n",
      "#59-E]\n",
      "708.50\n",
      "#\n",
      "#\n",
      "#36-HT\n",
      "4882\n",
      "#am 59-51\n",
      "098.41)\n",
      "#81-61\n",
      "1135.56\n",
      "#60-F1\n",
      "506.70\n",
      "#63-/\n",
      "0200\n",
      "#\n",
      "#\n",
      "#55-79\n",
      "7840)\n",
      "#\n",
      "#\n"
     ]
    }
   ],
   "source": [
    "for i in list_o:\n",
    "    print('#{}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "unrecognized image mode",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-a657ba8f0905>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# ret, gray = cv2.threshold(closing, 180, 255,cv2.THRESH_BINARY)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'uint8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'gray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpytesseract\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_to_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   2534\u001b[0m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2536\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfrombuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"raw\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mfrombuffer\u001b[1;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[0;32m   2477\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2478\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2479\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfrombytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mfrombytes\u001b[1;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[0;32m   2409\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2411\u001b[1;33m     \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2412\u001b[0m     \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrombytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2413\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mnew\u001b[1;34m(mode, size, color)\u001b[0m\n\u001b[0;32m   2373\u001b[0m         \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageColor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcolor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2375\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: unrecognized image mode"
     ]
    }
   ],
   "source": [
    "img_path = 'crop_image\\\\1.jpg'\n",
    "image = cv2.imread(img_path,0)\n",
    "# plt.imshow(image, cmap = 'gray')\n",
    "# hist,bins = np.histogram(image.flatten(),256,[0,255]) \n",
    "# plt.imshow(hist,cmap = 'gray')\n",
    "img = cv2.equalizeHist(image)\n",
    "ret, img = cv2.threshold(img, 180, 255,cv2.THRESH_BINARY)\n",
    "\n",
    "kernel = np.ones((3,3),np.uint8)\n",
    "closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)\n",
    "# ret, gray = cv2.threshold(closing, 180, 255,cv2.THRESH_BINARY)\n",
    "test = Image.fromarray(closing.astype('uint8'), 'gray')\n",
    "text = pytesseract.image_to_string(Image.open(closing))\n",
    "print(text)\n",
    "plt.imshow(closing, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = 'crop_image\\\\'\n",
    "# img = cv2.imread(img_path,0)\n",
    "# ret, gray = cv2.threshold(img, 150, 255,cv2.THRESH_BINARY)\n",
    "# plt.imshow(gray, cmap = 'gray')\n",
    "def OCR_test(img_path = '', pre = 'thresh', save_path = '', number_pics = ''):\n",
    "    image = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    if pre == 'thresh':\n",
    "        ret, gray = cv2.threshold(gray, 180, 255,cv2.THRESH_BINARY)\n",
    "#     elif pre =='blur':\n",
    "#         gray = cv2.medianBlur(gray, 3)\n",
    "    save_into = save_path+number_pics+'.jpg'\n",
    "    cv2.imwrite(save_into, gray) \n",
    "count = '1'\n",
    "count_str = 1\n",
    "for i in os.listdir(img_list):\n",
    "    img_path = img_list+i\n",
    "    OCR_test(img_path = img_path, pre = 'thresh', save_path = 'preprocess_test\\\\', number_pics = count)\n",
    "    count_str += 1\n",
    "    count = str(count_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#59-P1\n",
      "661.80\n",
      "#\n",
      "#FT,\n",
      "170.78\n",
      "#\n",
      "#59-1\n",
      "98.97]\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#59-B1 |\n",
      "38559\n",
      "#50H\n",
      "6882 fi\n",
      "#\n",
      "#9-51 |\n",
      "098.41]\n",
      "#\n",
      "#§0-F\n",
      "506.70)\n",
      "#63-41]\n",
      "02004\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#ae “TT\n",
      "#\n",
      "#\n",
      "#\n",
      "#59-ul\n",
      "136.29\n",
      "#1-C1\n",
      "789.15\n",
      "#\n"
     ]
    }
   ],
   "source": [
    "list_test = []\n",
    "for i in range(28):\n",
    "    name = i+1\n",
    "    test = 'preprocess_test\\\\'+ str(name) + '.jpg'\n",
    "    list_test.append(pytesseract.image_to_string(Image.open(test)))\n",
    "for i in list_test:\n",
    "    print('#{}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Using real-time data augmentation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`steps_per_epoch=None` is only valid for a generator based on the `keras.utils.Sequence` class. Please specify `steps_per_epoch` or use the `keras.utils.Sequence` class.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8a3414cf39f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    117\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m                         workers=4)\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;31m# Save model and weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             raise ValueError('`steps_per_epoch=None` is only valid for a'\n\u001b[0m\u001b[0;32m     56\u001b[0m                              \u001b[1;34m' generator based on the '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m                              \u001b[1;34m'`keras.utils.Sequence`'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: `steps_per_epoch=None` is only valid for a generator based on the `keras.utils.Sequence` class. Please specify `steps_per_epoch` or use the `keras.utils.Sequence` class."
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Train a simple deep CNN on the CIFAR10 small images dataset.\n",
    "\n",
    "It gets to 75% validation accuracy in 25 epochs, and 79% after 50 epochs.\n",
    "(it's still underfitting at that point, though).\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "img = Image.open('five.jpg')\n",
    "# print(np.array(img).shape)\n",
    "image = img.resize((28,28), Image.LANCZOS)\n",
    "image = np.array(image)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[[1,2,3],[1,2,3]],[[1,2,3],[1,2,3]]])\n",
    "b = np.array([[[1,2,3],[1,2,3]],[[1,2,3],[1,2,3]],[[1,2,3],[1,2,3]]])\n",
    "\n",
    "print(np.concatenate((a,b),axis=0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 9.293680297397769 %\n",
      "Done 18.587360594795538 %\n",
      "Done 27.881040892193308 %\n",
      "Done 37.174721189591075 %\n",
      "Done 46.468401486988846 %\n",
      "Done 55.762081784386616 %\n",
      "Done 65.05576208178438 %\n",
      "Done 74.34944237918215 %\n",
      "Done 83.64312267657992 %\n",
      "Done 92.93680297397769 %\n",
      "Done 100 %\n"
     ]
    }
   ],
   "source": [
    "# Take hog vetor and set label for all data positive\n",
    "positive_path = '.\\\\charTrainset\\\\'\n",
    "X_train_positive = np.zeros((1076,672))\n",
    "label_positive = []\n",
    "count = 0\n",
    "for digits in os.listdir(positive_path):\n",
    "    for pics in os.listdir(positive_path+digits):\n",
    "        image = cv2.imread(positive_path+digits+'\\\\'+pics)\n",
    "        X_train_positive[count], hog_image = hog(image,orientations=8, pixels_per_cell=(2, 2), cells_per_block=(1, 1), \n",
    "                                        visualise = True,multichannel=True,feature_vector = True)\n",
    "        label_positive.append(1)\n",
    "        count+=1\n",
    "        if count%100==0:\n",
    "            print('Done {} %'.format((count*100)/1076))\n",
    "print('Done 100 %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 1.075268817204301 %\n",
      "Done 2.150537634408602 %\n",
      "Done 3.225806451612903 %\n",
      "Done 4.301075268817204 %\n",
      "Done 5.376344086021505 %\n",
      "Done 6.451612903225806 %\n",
      "Done 7.526881720430108 %\n",
      "Done 8.602150537634408 %\n",
      "Done 9.67741935483871 %\n",
      "Done 10.75268817204301 %\n",
      "Done 11.827956989247312 %\n",
      "Done 12.903225806451612 %\n",
      "Done 13.978494623655914 %\n",
      "Done 15.053763440860216 %\n",
      "Done 16.129032258064516 %\n",
      "Done 17.204301075268816 %\n",
      "Done 18.27956989247312 %\n",
      "Done 19.35483870967742 %\n",
      "Done 20.43010752688172 %\n",
      "Done 21.50537634408602 %\n",
      "Done 22.580645161290324 %\n",
      "Done 23.655913978494624 %\n",
      "Done 24.731182795698924 %\n",
      "Done 25.806451612903224 %\n",
      "Done 26.881720430107528 %\n",
      "Done 27.956989247311828 %\n",
      "Done 29.032258064516128 %\n",
      "Done 30.107526881720432 %\n",
      "Done 31.182795698924732 %\n",
      "Done 32.25806451612903 %\n",
      "Done 33.333333333333336 %\n",
      "Done 34.40860215053763 %\n",
      "Done 35.483870967741936 %\n",
      "Done 36.55913978494624 %\n",
      "Done 37.634408602150536 %\n",
      "Done 38.70967741935484 %\n",
      "Done 39.784946236559136 %\n",
      "Done 40.86021505376344 %\n",
      "Done 41.935483870967744 %\n",
      "Done 43.01075268817204 %\n",
      "Done 44.086021505376344 %\n",
      "Done 45.16129032258065 %\n",
      "Done 46.236559139784944 %\n",
      "Done 47.31182795698925 %\n",
      "Done 48.38709677419355 %\n",
      "Done 49.46236559139785 %\n",
      "Done 50.53763440860215 %\n",
      "Done 51.61290322580645 %\n",
      "Done 52.68817204301075 %\n",
      "Done 53.763440860215056 %\n",
      "Done 54.83870967741935 %\n",
      "Done 55.913978494623656 %\n",
      "Done 56.98924731182796 %\n",
      "Done 58.064516129032256 %\n",
      "Done 59.13978494623656 %\n",
      "Done 60.215053763440864 %\n",
      "Done 61.29032258064516 %\n",
      "Done 62.365591397849464 %\n",
      "Done 63.44086021505376 %\n",
      "Done 64.51612903225806 %\n",
      "Done 65.59139784946237 %\n",
      "Done 66.66666666666667 %\n",
      "Done 67.74193548387096 %\n",
      "Done 68.81720430107526 %\n",
      "Done 69.89247311827957 %\n",
      "Done 70.96774193548387 %\n",
      "Done 72.04301075268818 %\n",
      "Done 73.11827956989248 %\n",
      "Done 74.19354838709677 %\n",
      "Done 75.26881720430107 %\n",
      "Done 76.34408602150538 %\n",
      "Done 77.41935483870968 %\n",
      "Done 78.49462365591398 %\n",
      "Done 79.56989247311827 %\n",
      "Done 80.64516129032258 %\n",
      "Done 81.72043010752688 %\n",
      "Done 82.79569892473118 %\n",
      "Done 83.87096774193549 %\n",
      "Done 84.94623655913979 %\n",
      "Done 86.02150537634408 %\n",
      "Done 87.09677419354838 %\n",
      "Done 88.17204301075269 %\n",
      "Done 89.24731182795699 %\n",
      "Done 90.3225806451613 %\n",
      "Done 91.39784946236558 %\n",
      "Done 92.47311827956989 %\n",
      "Done 93.54838709677419 %\n",
      "Done 94.6236559139785 %\n",
      "Done 95.6989247311828 %\n",
      "Done 96.7741935483871 %\n",
      "Done 97.84946236559139 %\n",
      "Done 98.9247311827957 %\n",
      "Done 100.0 %\n",
      "Done 100 %\n"
     ]
    }
   ],
   "source": [
    "# Take hog vetor and set label for all data negative\n",
    "negative_path1 = ['.\\\\training-samples\\\\detection\\\\negatives\\\\','.\\\\training-samples\\\\icdar\\\\negatives\\\\']\n",
    "X_train_negative = np.zeros((9300,672))\n",
    "label_negative = []\n",
    "count = 0\n",
    "for path in negative_path1:\n",
    "    for pics in os.listdir(path):\n",
    "        img = Image.open(path+pics)\n",
    "        image = img.resize((12,28))\n",
    "        image_np = np.array(image)\n",
    "        X_train_negative[count], hog_image = hog(image_np,orientations=8, pixels_per_cell=(2, 2), cells_per_block=(1, 1), \n",
    "                                        visualise = True,multichannel=True,feature_vector = True)\n",
    "        label_negative.append(0)\n",
    "        count+=1\n",
    "        if count%100==0:\n",
    "            print('Done {} %'.format((count*100)/9300))\n",
    "print('Done 100 %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine together\n",
    "X_train = np.concatenate((X_train_positive,X_train_negative), axis = 0)\n",
    "y_train = label_positive + label_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 88.49557522123894 %\n",
      "Done 100 %\n"
     ]
    }
   ],
   "source": [
    "# Take hog vetor and set label for all test data negative\n",
    "test_path = '.\\\\test_data\\\\negative\\\\'\n",
    "X_test_negative = np.zeros((113,672))\n",
    "label_test_negative = []\n",
    "count = 0\n",
    "for pics in os.listdir(test_path):\n",
    "    img = Image.open(test_path+pics)\n",
    "    image = img.resize((12,28))\n",
    "    image_np = np.array(image)\n",
    "    X_test_negative[count], hog_image = hog(image_np,orientations=8, pixels_per_cell=(2, 2), cells_per_block=(1, 1), \n",
    "                                    visualise = True,multichannel=True,feature_vector = True)\n",
    "    label_test_negative.append(0)\n",
    "    count+=1\n",
    "    if count%100==0:\n",
    "        print('Done {} %'.format((count*100)/113))\n",
    "print('Done 100 %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take hog vetor and set label for all test data positive\n",
    "test_path = 'C:\\\\Users\\\\ADMINS\\\\Desktop\\\\train_32x32.mat'\n",
    "dict_mat = scipy.io.loadmat(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "all_pics = dict_mat['X']\n",
    "shape = all_pics.shape[3]\n",
    "X_test_positive = np.zeros((100,672))\n",
    "label_test_positive = []\n",
    "for count in range(100):\n",
    "    img = all_pics[:,:,:,count]\n",
    "    img = Image.fromarray(img)\n",
    "    image = img.resize((12,28))\n",
    "    image_np = np.array(image)\n",
    "    X_test_positive[count], hog_image = hog(image_np,orientations=8, pixels_per_cell=(2, 2), cells_per_block=(1, 1), \n",
    "                                    visualise = True,multichannel=True,feature_vector = True)\n",
    "    label_test_positive.append(1)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine together and then save to pickle file\n",
    "X_test = np.concatenate((X_test_positive,X_test_negative), axis = 0)\n",
    "y_test = label_test_positive + label_test_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save all data of hog vector into mat file\n",
    "data_dict = {'X_train':X_train,'y_train':y_train,'X_test':X_test,'y_test':y_test}\n",
    "scipy.io.savemat('dataTraining',data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10376, 672) (1, 10376) (213, 672) (1, 213)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#load data from mat file\n",
    "data_path = '.\\\\dataTraining.mat'\n",
    "data = scipy.io.loadmat(data_path)\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "X_test  = data['X_test']\n",
    "y_test  = data['y_test']\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10376, 2)\n",
      "Epoch 1/10\n",
      "10376/10376 [==============================] - 0s 39us/step - loss: 0.6985 - acc: 0.8871\n",
      "Epoch 2/10\n",
      "10376/10376 [==============================] - 0s 6us/step - loss: 0.5072 - acc: 0.8963\n",
      "Epoch 3/10\n",
      "10376/10376 [==============================] - 0s 5us/step - loss: 0.5006 - acc: 0.8963\n",
      "Epoch 4/10\n",
      "10376/10376 [==============================] - 0s 6us/step - loss: 0.5006 - acc: 0.8963\n",
      "Epoch 5/10\n",
      "10376/10376 [==============================] - 0s 5us/step - loss: 0.5005 - acc: 0.8963\n",
      "Epoch 6/10\n",
      "10376/10376 [==============================] - 0s 5us/step - loss: 0.5005 - acc: 0.8963\n",
      "Epoch 7/10\n",
      "10376/10376 [==============================] - 0s 5us/step - loss: 0.5005 - acc: 0.8963\n",
      "Epoch 8/10\n",
      "10376/10376 [==============================] - 0s 5us/step - loss: 0.5005 - acc: 0.8963\n",
      "Epoch 9/10\n",
      "10376/10376 [==============================] - 0s 6us/step - loss: 0.5004 - acc: 0.8963\n",
      "Epoch 10/10\n",
      "10376/10376 [==============================] - 0s 5us/step - loss: 0.5004 - acc: 0.8963\n",
      "Test loss: 0.5\n",
      "Test accuracy: 0.5305164319248826\n"
     ]
    }
   ],
   "source": [
    "# Training window for recognize digits in license plate\n",
    "batch_size = 1860\n",
    "num_classes = 2\n",
    "epochs = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "print(y_train.shape)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "model = Sequential()\n",
    "model.add(Dense(60, activation='relu'))\n",
    "model.add(Dense(150, activation='relu'))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(num_classes, activation = 'relu'))\n",
    "model.compile(loss='hinge',\n",
    "              optimizer=keras.optimizers.Adam(lr=0.001,beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1)\n",
    "#           validation_data=(X_test, y_test))\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x27ac0ba72e8>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2 \n",
    "img = Image.open('test_license.jpg')\n",
    "image = img.resize((120,50))\n",
    "image = np.array(image)\n",
    "a = (image[0:28,18:30])\n",
    "b, hog_image = hog(image_np,orientations=8, pixels_per_cell=(2, 2), cells_per_block=(1, 1), \n",
    "                                    visualise = True,multichannel=True,feature_vector = True) \n",
    "b = np.array(b,ndmin=2)\n",
    "plt.imshow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-80a2803756d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m __all__ += [name for name in dir(_C)\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-e83f1f95b46b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m __all__ += [name for name in dir(_C)\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "# Why I use pytorch? Because it is much more easier to debug if we compare with tensorflow (Good job M.Z!)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(672, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    # Look again about data, find more about torch data\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # torch.optim as optim on above\n",
    "        optimizer.zero_grad()\n",
    "        # putdata into model\n",
    "        output = model(data)\n",
    "        # loss function of course\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # and we take backprop from loss function (more clearly than tensorflow ... or keras)\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "        # should better use entropy loss, keyword : torch.nn.CrossEntropyLoss\n",
    "\n",
    "net = Net()    \n",
    "params = list(net.parameters())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
