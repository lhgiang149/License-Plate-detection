{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from skimage.feature import hog\n",
    "from PIL import Image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "import keras\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OCR_scan(img_path = '', pre = 'thresh', save_path = '', number_pics = ''):\n",
    "    image = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    if pre == 'thresh':\n",
    "        ret, gray = cv2.threshold(gray, 0, 255,cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "    elif pre =='blur':\n",
    "        gray = cv2.medianBlur(gray, 3)\n",
    "    save_into = save_path+number_pics+'.jpg'\n",
    "    cv2.imwrite(save_into, gray) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = 'crop_image\\\\'\n",
    "pre1 = 'thresh'\n",
    "pre2 = 'blur'\n",
    "save_path1 = 'preprocess_thresh\\\\'\n",
    "save_path2 = 'preprocess_blur\\\\'\n",
    "count = '1'\n",
    "count_str = 1\n",
    "for i in os.listdir(img_list):\n",
    "    img_path = img_list+i\n",
    "    OCR_scan(img_path = img_path, pre = pre1, save_path = save_path1, number_pics = count)\n",
    "    OCR_scan(img_path = img_path, pre = pre2, save_path = save_path2, number_pics = count)\n",
    "    count_str += 1\n",
    "    count = str(count_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'preprocess_thresh\\\\'\n",
    "list_thresh = []\n",
    "list_blur = []\n",
    "list_o = []\n",
    "list_test = []\n",
    "for i in range(28):\n",
    "    name = i+1\n",
    "    thresh = filename + str(name) + '.jpg'\n",
    "    blur = 'preprocess_blur\\\\'+ str(name) + '.jpg'\n",
    "    original = 'crop_image\\\\'+ str(name) + '.jpg'\n",
    "    test = 'preprocess_test\\\\'+ str(name) + '.jpg'\n",
    "    list_thresh.append(pytesseract.image_to_string(Image.open(thresh)))\n",
    "    list_blur.append(pytesseract.image_to_string(Image.open(blur)))\n",
    "    list_o.append(pytesseract.image_to_string(Image.open(original)))\n",
    "    list_test.append(pytesseract.image_to_string(Image.open(test)))\n",
    "\n",
    "# print(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of thresholding is: 63-89\n",
      "99999\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_path = 'preprocess_blur\\\\1.jpg'\n",
    "img_path = 'test_license.jpg'\n",
    "img_path = 'something.jpg'\n",
    "# img_path = '.\\\\training-samples\\\\detection\\\\positives\\\\0_036_000.png'\n",
    "# img_path = 'test_tesseract1.png'\n",
    "image = Image.open(img_path)\n",
    "# image = image.resize((100,100))\n",
    "image = np.array(image)\n",
    "# image = image[5:95,5:95]\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "ret, test_1 = cv2.threshold(gray, 0, 255,cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "test_2 = cv2.medianBlur(gray, 9)\n",
    "kernel = np.ones((5,5),np.uint8)\n",
    "erosion = cv2.dilate(test_1,kernel,iterations = 1)\n",
    "plt.imshow(erosion,cmap = 'gray')\n",
    "config = ('-l eng --oem 1 --psm 3')\n",
    "a = (pytesseract.image_to_string(erosion,config = config))\n",
    "print('Result of thresholding is: {}'.format(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#J 59 PA j\n",
      "664.80\n",
      "#62 HS\n",
      "7108\n",
      "#\n",
      "#\n",
      "#1\n",
      "99.97\n",
      "#\n",
      "#ead\n",
      "043.09\n",
      "#\n",
      "#BY 59-41\n",
      "so7 00\n",
      "#59-81\n",
      "385.59\n",
      "#54-H}\n",
      "6982\n",
      "#\n",
      "#59°51\n",
      "09841\n",
      "#81-61\n",
      "135.56\n",
      "#60-F1\n",
      "506.70\n",
      "#Be \"1\n",
      "0200\n",
      "#16M\n",
      "06.\n",
      "#\n",
      "#S5-Po i\n",
      "2840\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#136.29\n",
      "#\n",
      "#59-(2\n",
      "316.61\n"
     ]
    }
   ],
   "source": [
    "for i in list_thresh:\n",
    "    print('#{}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#59897\n",
      "#B 99°C2\n",
      "012.08\n",
      "#ie\n",
      "043.09\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#39-51\n",
      "098.41\n",
      "#81-61\n",
      "135.56\n",
      "#60:F1\n",
      "506.70\n",
      "#\n",
      "#1\n",
      "0\n",
      "\n",
      "6-H\n",
      "61\n",
      "\n",
      "i\n",
      "#\n",
      "#Gar) ¢\n",
      "7840)\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#136.29\n",
      "#70-01\n",
      "/39 16\n",
      "#39-(2\n",
      "316.01\n"
     ]
    }
   ],
   "source": [
    "for i in list_blur:\n",
    "    print('#{}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#, 59-P1 '\n",
      "664.80\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#136.29\n",
      "#\n",
      "#59-(7\n",
      "316.01\n",
      "#\n",
      "#\n",
      "#\n",
      "#U1]\n",
      "98.97\n",
      "#\n",
      "#78-G)\n",
      "043.08)\n",
      "#59-E]\n",
      "708.50\n",
      "#\n",
      "#\n",
      "#36-HT\n",
      "4882\n",
      "#am 59-51\n",
      "098.41)\n",
      "#81-61\n",
      "1135.56\n",
      "#60-F1\n",
      "506.70\n",
      "#63-/\n",
      "0200\n",
      "#\n",
      "#\n",
      "#55-79\n",
      "7840)\n",
      "#\n",
      "#\n"
     ]
    }
   ],
   "source": [
    "for i in list_o:\n",
    "    print('#{}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "unrecognized image mode",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-a657ba8f0905>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# ret, gray = cv2.threshold(closing, 180, 255,cv2.THRESH_BINARY)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'uint8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'gray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpytesseract\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_to_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   2534\u001b[0m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2536\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfrombuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"raw\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mfrombuffer\u001b[1;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[0;32m   2477\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2478\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2479\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfrombytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mfrombytes\u001b[1;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[0;32m   2409\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2411\u001b[1;33m     \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2412\u001b[0m     \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrombytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2413\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mnew\u001b[1;34m(mode, size, color)\u001b[0m\n\u001b[0;32m   2373\u001b[0m         \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageColor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcolor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2375\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: unrecognized image mode"
     ]
    }
   ],
   "source": [
    "img_path = 'crop_image\\\\1.jpg'\n",
    "image = cv2.imread(img_path,0)\n",
    "# plt.imshow(image, cmap = 'gray')\n",
    "# hist,bins = np.histogram(image.flatten(),256,[0,255]) \n",
    "# plt.imshow(hist,cmap = 'gray')\n",
    "img = cv2.equalizeHist(image)\n",
    "ret, img = cv2.threshold(img, 180, 255,cv2.THRESH_BINARY)\n",
    "\n",
    "kernel = np.ones((3,3),np.uint8)\n",
    "closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)\n",
    "# ret, gray = cv2.threshold(closing, 180, 255,cv2.THRESH_BINARY)\n",
    "test = Image.fromarray(closing.astype('uint8'), 'gray')\n",
    "text = pytesseract.image_to_string(Image.open(closing))\n",
    "print(text)\n",
    "plt.imshow(closing, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = 'crop_image\\\\'\n",
    "# img = cv2.imread(img_path,0)\n",
    "# ret, gray = cv2.threshold(img, 150, 255,cv2.THRESH_BINARY)\n",
    "# plt.imshow(gray, cmap = 'gray')\n",
    "def OCR_test(img_path = '', pre = 'thresh', save_path = '', number_pics = ''):\n",
    "    image = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    if pre == 'thresh':\n",
    "        ret, gray = cv2.threshold(gray, 180, 255,cv2.THRESH_BINARY)\n",
    "#     elif pre =='blur':\n",
    "#         gray = cv2.medianBlur(gray, 3)\n",
    "    save_into = save_path+number_pics+'.jpg'\n",
    "    cv2.imwrite(save_into, gray) \n",
    "count = '1'\n",
    "count_str = 1\n",
    "for i in os.listdir(img_list):\n",
    "    img_path = img_list+i\n",
    "    OCR_test(img_path = img_path, pre = 'thresh', save_path = 'preprocess_test\\\\', number_pics = count)\n",
    "    count_str += 1\n",
    "    count = str(count_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#59-P1\n",
      "661.80\n",
      "#\n",
      "#FT,\n",
      "170.78\n",
      "#\n",
      "#59-1\n",
      "98.97]\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#59-B1 |\n",
      "38559\n",
      "#50H\n",
      "6882 fi\n",
      "#\n",
      "#9-51 |\n",
      "098.41]\n",
      "#\n",
      "#§0-F\n",
      "506.70)\n",
      "#63-41]\n",
      "02004\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#ae “TT\n",
      "#\n",
      "#\n",
      "#\n",
      "#59-ul\n",
      "136.29\n",
      "#1-C1\n",
      "789.15\n",
      "#\n"
     ]
    }
   ],
   "source": [
    "list_test = []\n",
    "for i in range(28):\n",
    "    name = i+1\n",
    "    test = 'preprocess_test\\\\'+ str(name) + '.jpg'\n",
    "    list_test.append(pytesseract.image_to_string(Image.open(test)))\n",
    "for i in list_test:\n",
    "    print('#{}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Using real-time data augmentation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`steps_per_epoch=None` is only valid for a generator based on the `keras.utils.Sequence` class. Please specify `steps_per_epoch` or use the `keras.utils.Sequence` class.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8a3414cf39f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    117\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m                         workers=4)\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;31m# Save model and weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             raise ValueError('`steps_per_epoch=None` is only valid for a'\n\u001b[0m\u001b[0;32m     56\u001b[0m                              \u001b[1;34m' generator based on the '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m                              \u001b[1;34m'`keras.utils.Sequence`'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: `steps_per_epoch=None` is only valid for a generator based on the `keras.utils.Sequence` class. Please specify `steps_per_epoch` or use the `keras.utils.Sequence` class."
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Train a simple deep CNN on the CIFAR10 small images dataset.\n",
    "\n",
    "It gets to 75% validation accuracy in 25 epochs, and 79% after 50 epochs.\n",
    "(it's still underfitting at that point, though).\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "img = Image.open('five.jpg')\n",
    "# print(np.array(img).shape)\n",
    "image = img.resize((28,28), Image.LANCZOS)\n",
    "image = np.array(image)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[[1,2,3],[1,2,3]],[[1,2,3],[1,2,3]]])\n",
    "b = np.array([[[1,2,3],[1,2,3]],[[1,2,3],[1,2,3]],[[1,2,3],[1,2,3]]])\n",
    "\n",
    "print(np.concatenate((a,b),axis=0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 9.293680297397769 %\n",
      "Done 18.587360594795538 %\n",
      "Done 27.881040892193308 %\n",
      "Done 37.174721189591075 %\n",
      "Done 46.468401486988846 %\n",
      "Done 55.762081784386616 %\n",
      "Done 65.05576208178438 %\n",
      "Done 74.34944237918215 %\n",
      "Done 83.64312267657992 %\n",
      "Done 92.93680297397769 %\n",
      "Done 100 %\n"
     ]
    }
   ],
   "source": [
    "# Take hog vetor and set label for all data positive\n",
    "positive_path = '.\\\\charTrainset\\\\'\n",
    "X_train_positive = np.zeros((1076,672))\n",
    "label_positive = []\n",
    "count = 0\n",
    "for digits in os.listdir(positive_path):\n",
    "    for pics in os.listdir(positive_path+digits):\n",
    "        image = cv2.imread(positive_path+digits+'\\\\'+pics)\n",
    "        X_train_positive[count], hog_image = hog(image,orientations=8, pixels_per_cell=(2, 2), cells_per_block=(1, 1), \n",
    "                                        visualise = True,multichannel=True,feature_vector = True)\n",
    "        label_positive.append(1)\n",
    "        count+=1\n",
    "        if count%100==0:\n",
    "            print('Done {} %'.format((count*100)/1076))\n",
    "print('Done 100 %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 1.075268817204301 %\n",
      "Done 2.150537634408602 %\n",
      "Done 3.225806451612903 %\n",
      "Done 4.301075268817204 %\n",
      "Done 5.376344086021505 %\n",
      "Done 6.451612903225806 %\n",
      "Done 7.526881720430108 %\n",
      "Done 8.602150537634408 %\n",
      "Done 9.67741935483871 %\n",
      "Done 10.75268817204301 %\n",
      "Done 11.827956989247312 %\n",
      "Done 12.903225806451612 %\n",
      "Done 13.978494623655914 %\n",
      "Done 15.053763440860216 %\n",
      "Done 16.129032258064516 %\n",
      "Done 17.204301075268816 %\n",
      "Done 18.27956989247312 %\n",
      "Done 19.35483870967742 %\n",
      "Done 20.43010752688172 %\n",
      "Done 21.50537634408602 %\n",
      "Done 22.580645161290324 %\n",
      "Done 23.655913978494624 %\n",
      "Done 24.731182795698924 %\n",
      "Done 25.806451612903224 %\n",
      "Done 26.881720430107528 %\n",
      "Done 27.956989247311828 %\n",
      "Done 29.032258064516128 %\n",
      "Done 30.107526881720432 %\n",
      "Done 31.182795698924732 %\n",
      "Done 32.25806451612903 %\n",
      "Done 33.333333333333336 %\n",
      "Done 34.40860215053763 %\n",
      "Done 35.483870967741936 %\n",
      "Done 36.55913978494624 %\n",
      "Done 37.634408602150536 %\n",
      "Done 38.70967741935484 %\n",
      "Done 39.784946236559136 %\n",
      "Done 40.86021505376344 %\n",
      "Done 41.935483870967744 %\n",
      "Done 43.01075268817204 %\n",
      "Done 44.086021505376344 %\n",
      "Done 45.16129032258065 %\n",
      "Done 46.236559139784944 %\n",
      "Done 47.31182795698925 %\n",
      "Done 48.38709677419355 %\n",
      "Done 49.46236559139785 %\n",
      "Done 50.53763440860215 %\n",
      "Done 51.61290322580645 %\n",
      "Done 52.68817204301075 %\n",
      "Done 53.763440860215056 %\n",
      "Done 54.83870967741935 %\n",
      "Done 55.913978494623656 %\n",
      "Done 56.98924731182796 %\n",
      "Done 58.064516129032256 %\n",
      "Done 59.13978494623656 %\n",
      "Done 60.215053763440864 %\n",
      "Done 61.29032258064516 %\n",
      "Done 62.365591397849464 %\n",
      "Done 63.44086021505376 %\n",
      "Done 64.51612903225806 %\n",
      "Done 65.59139784946237 %\n",
      "Done 66.66666666666667 %\n",
      "Done 67.74193548387096 %\n",
      "Done 68.81720430107526 %\n",
      "Done 69.89247311827957 %\n",
      "Done 70.96774193548387 %\n",
      "Done 72.04301075268818 %\n",
      "Done 73.11827956989248 %\n",
      "Done 74.19354838709677 %\n",
      "Done 75.26881720430107 %\n",
      "Done 76.34408602150538 %\n",
      "Done 77.41935483870968 %\n",
      "Done 78.49462365591398 %\n",
      "Done 79.56989247311827 %\n",
      "Done 80.64516129032258 %\n",
      "Done 81.72043010752688 %\n",
      "Done 82.79569892473118 %\n",
      "Done 83.87096774193549 %\n",
      "Done 84.94623655913979 %\n",
      "Done 86.02150537634408 %\n",
      "Done 87.09677419354838 %\n",
      "Done 88.17204301075269 %\n",
      "Done 89.24731182795699 %\n",
      "Done 90.3225806451613 %\n",
      "Done 91.39784946236558 %\n",
      "Done 92.47311827956989 %\n",
      "Done 93.54838709677419 %\n",
      "Done 94.6236559139785 %\n",
      "Done 95.6989247311828 %\n",
      "Done 96.7741935483871 %\n",
      "Done 97.84946236559139 %\n",
      "Done 98.9247311827957 %\n",
      "Done 100.0 %\n",
      "Done 100 %\n"
     ]
    }
   ],
   "source": [
    "# Take hog vetor and set label for all data negative\n",
    "negative_path1 = ['.\\\\training-samples\\\\detection\\\\negatives\\\\','.\\\\training-samples\\\\icdar\\\\negatives\\\\']\n",
    "X_train_negative = np.zeros((9300,672))\n",
    "label_negative = []\n",
    "count = 0\n",
    "for path in negative_path1:\n",
    "    for pics in os.listdir(path):\n",
    "        img = Image.open(path+pics)\n",
    "        image = img.resize((12,28))\n",
    "        image_np = np.array(image)\n",
    "        X_train_negative[count], hog_image = hog(image_np,orientations=8, pixels_per_cell=(2, 2), cells_per_block=(1, 1), \n",
    "                                        visualise = True,multichannel=True,feature_vector = True)\n",
    "        label_negative.append(0)\n",
    "        count+=1\n",
    "        if count%100==0:\n",
    "            print('Done {} %'.format((count*100)/9300))\n",
    "print('Done 100 %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine together\n",
    "X_train = np.concatenate((X_train_positive,X_train_negative), axis = 0)\n",
    "y_train = label_positive + label_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 88.49557522123894 %\n",
      "Done 100 %\n"
     ]
    }
   ],
   "source": [
    "# Take hog vetor and set label for all test data negative\n",
    "test_path = '.\\\\test_data\\\\negative\\\\'\n",
    "X_test_negative = np.zeros((113,672))\n",
    "label_test_negative = []\n",
    "count = 0\n",
    "for pics in os.listdir(test_path):\n",
    "    img = Image.open(test_path+pics)\n",
    "    image = img.resize((12,28))\n",
    "    image_np = np.array(image)\n",
    "    X_test_negative[count], hog_image = hog(image_np,orientations=8, pixels_per_cell=(2, 2), cells_per_block=(1, 1), \n",
    "                                    visualise = True,multichannel=True,feature_vector = True)\n",
    "    label_test_negative.append(0)\n",
    "    count+=1\n",
    "    if count%100==0:\n",
    "        print('Done {} %'.format((count*100)/113))\n",
    "print('Done 100 %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take hog vetor and set label for all test data positive\n",
    "test_path = 'C:\\\\Users\\\\ADMINS\\\\Desktop\\\\train_32x32.mat'\n",
    "dict_mat = scipy.io.loadmat(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "all_pics = dict_mat['X']\n",
    "shape = all_pics.shape[3]\n",
    "X_test_positive = np.zeros((100,672))\n",
    "label_test_positive = []\n",
    "for count in range(100):\n",
    "    img = all_pics[:,:,:,count]\n",
    "    img = Image.fromarray(img)\n",
    "    image = img.resize((12,28))\n",
    "    image_np = np.array(image)\n",
    "    X_test_positive[count], hog_image = hog(image_np,orientations=8, pixels_per_cell=(2, 2), cells_per_block=(1, 1), \n",
    "                                    visualise = True,multichannel=True,feature_vector = True)\n",
    "    label_test_positive.append(1)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine together and then save to pickle file\n",
    "X_test = np.concatenate((X_test_positive,X_test_negative), axis = 0)\n",
    "y_test = label_test_positive + label_test_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save all data of hog vector into mat file\n",
    "data_dict = {'X_train':X_train,'y_train':y_train,'X_test':X_test,'y_test':y_test}\n",
    "scipy.io.savemat('dataTraining',data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10376, 672) (1, 10376) (213, 672) (1, 213)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#load data from mat file\n",
    "data_path = '.\\\\dataTraining.mat'\n",
    "data = scipy.io.loadmat(data_path)\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "X_test  = data['X_test']\n",
    "y_test  = data['y_test']\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10376, 2)\n",
      "Epoch 1/10\n",
      "10376/10376 [==============================] - 0s 39us/step - loss: 0.6985 - acc: 0.8871\n",
      "Epoch 2/10\n",
      "10376/10376 [==============================] - 0s 6us/step - loss: 0.5072 - acc: 0.8963\n",
      "Epoch 3/10\n",
      "10376/10376 [==============================] - 0s 5us/step - loss: 0.5006 - acc: 0.8963\n",
      "Epoch 4/10\n",
      "10376/10376 [==============================] - 0s 6us/step - loss: 0.5006 - acc: 0.8963\n",
      "Epoch 5/10\n",
      "10376/10376 [==============================] - 0s 5us/step - loss: 0.5005 - acc: 0.8963\n",
      "Epoch 6/10\n",
      "10376/10376 [==============================] - 0s 5us/step - loss: 0.5005 - acc: 0.8963\n",
      "Epoch 7/10\n",
      "10376/10376 [==============================] - 0s 5us/step - loss: 0.5005 - acc: 0.8963\n",
      "Epoch 8/10\n",
      "10376/10376 [==============================] - 0s 5us/step - loss: 0.5005 - acc: 0.8963\n",
      "Epoch 9/10\n",
      "10376/10376 [==============================] - 0s 6us/step - loss: 0.5004 - acc: 0.8963\n",
      "Epoch 10/10\n",
      "10376/10376 [==============================] - 0s 5us/step - loss: 0.5004 - acc: 0.8963\n",
      "Test loss: 0.5\n",
      "Test accuracy: 0.5305164319248826\n"
     ]
    }
   ],
   "source": [
    "# Training window for recognize digits in license plate\n",
    "batch_size = 1860\n",
    "num_classes = 2\n",
    "epochs = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "print(y_train.shape)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "model = Sequential()\n",
    "model.add(Dense(60, activation='relu'))\n",
    "model.add(Dense(150, activation='relu'))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(num_classes, activation = 'relu'))\n",
    "model.compile(loss='hinge',\n",
    "              optimizer=keras.optimizers.Adam(lr=0.001,beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1)\n",
    "#           validation_data=(X_test, y_test))\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x27ac0ba72e8>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIIAAAD8CAYAAAC7K3xMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADl1JREFUeJztnW2MnNV1x39nZvbFS3aJndguxRioY8WxcOJICAW5HxJVpKSqMPnQKHyo/CGKSxta0vYLMgiQUlEU5UVRRFCd1opLeUnVlOAqqA3aD4S6L8KEJHaUF9PUMZZdvxG8azvs7Mycfti71mL2OXc8z7ys0f8nreblPPe5Z2f+c2fuec4919wdISqDdkAsDSQEAUgIIiEhCEBCEAkJQQASgkhICAKQEESi1s/OxicmfOXKlcUH9DDKaRbbc133NgIbO1fG91OnTjI9PZ05Q0khmNmtwFeAKvC37v5wdPzKlSt56KHiQxqzs5n+iv9jy7xa1Wpsn51thvZWq1XKHvlXqcRvQ+5/i/p+4IH7w7YXfGjrqEUwsyrwCPAxYCNwh5lt7PR8YrCU+Y1wE/CKu//C3evAU8DW7rgl+k0ZIVwNvLrg8ZH03Jsws+1mts/M9k1PTZXoTvSSMkJY7IvrLV/i7r7T3W909xvHJyZKdCd6SRkhHAGuWfB4DXC0nDtiUJQRwovAejO73syGgU8Ce7rjlug3HU8f3b1hZncB/8bc9HGXu/+4lDO12J2hoWqhrdmMp3+Qmd5V48/E0FC5KV5Eo1EP7bXacGj3VrG9Xb9KxRHc/Vng2TLnEEsDhZgFICGIhIQgAAlBJCQEAUgIItHXfATDwkuu1Vp8zT+6DO2tRti2FsQgAJiJ+x5ZNhS3z9AI4hwjw1fGbRvx/1bP2NtBI4IAJASRkBAEICGIhIQgAAlBJPo6fXRaNFszhfazZ86F7ffu3Vtoe+yx3WHboaF4+tesx5exK9V4+pm7hF4bLu7/a4/sDNuOj4+H9iuvHOnYr3k0IghAQhAJCUEAEoJISAgCkBBEQkIQQJ/jCK+dPs3jjxfP97/znX+NTxBcKc5dZq7X43T2SitO+254bjV07lJxcZzi7rvvDtvee++9oX1oqDiOMDNTHLdZiEYEAUgIIiEhCEBCEAkJQQASgkhICAIAK1M2zswOAdNAE2i4+43h8RXzahC5qFYyuvTIHrfNpYRHqfIA7rnl5XH/1UpxPkKrFretxSEM/vjOOwttTzz1Dxw//n+9La+X+Ii7n+rCecQA0VeDAMoLwYHvmtlLZra9Gw6JwVD2q2GLux81s1XAc2b2U3f/3sIDkkAkkiVOqRHB3Y+m2xPA08wV4bz4mAvl9TIlh8UAKVOC9wozG5+/D3wUONAtx0R/KfPVsBp4OlXtqgFPuHvmOrJYqpSKI1xyZ5WKR6XiapkK6tVgbcHMTFzZ/c/+9LOh/f0fuCG0u8frHvbv/2Fo/5udjxbaztfjgXk4M3CPLVtWaJs6+zqNxmz2S1nTRwFICCIhIQhAQhAJCUEAEoJI9DWdHQCLrqnGS9fHxt5RaFu//i2bx7yJD938lqDnRcTXenPTxxs2xdtZeZAOn0ulb1XiKf7ZX58vbpvZdOyCD20dJd72SAgCkBBEQkIQgIQgEhKCACQEkehzdXaoBCnrkS3Hpk2bQvvwcLxTWm6ntTK7uAHU68WXyVuZOEKOyDePagksQCOCACQEkZAQBCAhiISEIAAJQSQkBAEMIB8hnPNmUuujdPZcOfqoLUBuo7TcbvS5/pctGy20nT3bXs5AL9GIIAAJQSQkBAFICCIhIQhAQhAJCUEAbcQRzGwX8PvACXe/IT23AvgmcB1wCPiEu/+qnQ6jPHvLLIufbQRrA6pxvkEzk99vFscZqtVM+b3QCjYcfOY8LqVfsfht8i58nts5wzeAWy967h5g0t3XA5PpsbiMyQohFcd67aKntwLzO3DsBm7vsl+iz3Q6pqx292MA6XZV91wSg6Dn1xpUXu/yoNMR4biZXQWQbk8UHbiwvF7ZBFDROzoVwh5gW7q/DXimO+6IQZEVgpk9Cfwn8F4zO2JmnwIeBm4xs4PALemxuIzJ/kZw9zsKTL/TSYdlvh6ireuWBSXm2iGXC1GrxXGG3LZ60f+dW8+Re83Mitu3+2orsigACUEkJAQBSAgiISEIQEIQib6mszvxZehWM9bl6Eixu2Nj8fRx+fJ3hfYzr198Xe3NtPzXoX166kx8/pPnCm1Rxfo54telGxX2NSIIQEIQCQlBABKCSEgIApAQREJCEMAAyutFy9Mt487588Xl6Dds2BC2ve2220L71x75ami3Slx+78CBzre8LHsZWnEE0TUkBAFICCIhIQhAQhAJCUEAEoJI9He3eDOP8qur1eISdFCuxH92rk4cJ8iR21YvzMPwOFU+F0eIzt1szOKe3wdAI4IAJASRkBAEICGIhIQgAAlBJCQEAXReXu9B4NPAyXTYDnd/Nneu5cvfyS2/W7ya/lv/9C9h+0auln5Arky/ebnPhGfbF9tbxFsA5HzvBp2W1wP4srtvTn9ZEYilTafl9cTbjDLj4V1m9iMz22Vmy7vmkRgInQrhUWAdsBk4Bnyx6EAz225m+8xsX668jBgcHQnB3Y+7e9PdW8DXgZuCYy+U1xsZGenUT9FjOhLCfI3FxMeBzlN4xZKgnenjk8CHgXeb2RHgAeDDZraZuZXuh4A/6qGPog/0NR9h3bp1/tDDf11oz/2GOHz4cKFtcnIybLt3797YuVY8l5+dzW3313kNA6vE9RFy71GUr9Bo1PGW8hFEm0gIApAQREJCEICEIBISggD6PH18z3vW+ee/8PlCezWoMl6WXLo5lXiGde5ccXk8gDNn4vJ6999/f9B2OmybS8WPZr5KZxeXhIQgAAlBJCQEAUgIIiEhCEBCEIm+ltfL0fQ4XT03nw7b1nJLy+P2Y2Njob1ej5fVR+1zcYR8rKf8xqoaEQQgIYiEhCAACUEkJAQBSAgiISEIoN/b/bnTahbPt0dHV4Ttm83ZQtv7Nq4P2x48+LPQXm/GgQRrxjGOxuwboX166leFNs/0Xa3Fb1M9LBfQXr6JRgQBSAgiISEIQEIQCQlBABKCSEgIAmhjXYOZXQP8PfAbQAvY6e5fMbMVwDeB65irkfAJdy+eLAOVStVHhouvy+dyBlavXl1oe/7558O2IyPLQvt/vfgfob3+RvFWgwBHXv1laN+xY0ehbbYevwe596jZjOwt3L0r6xoawF+6+/uADwGfMbONwD3ApLuvBybTY3GZ0k55vWPu/v10fxr4CXA1sBXYnQ7bDdzeKydF77mk3whmdh3wQeC/gdXufgzmxAKs6rZzon+0fa3BzN4BfAv4rLtP5baXWdBuO7A9Pbp0D0VfaGtEMLMh5kTwuLv/c3r6+Hx1tXR7YrG2C8vrtSse0X+yQrC5d+/vgJ+4+5cWmPYA29L9bcAz3XdP9It2po+/DbwA7Gdu+giwg7nfCf8IrAUOA3/g7mHNZjPzSHrVSrzLWzSiDA3Flcnuu+++0P7EE4+F9l8e/t/Qfu5cnJIeVlj32PdcVfqokGl95jytVjM7FGd/I7j7v1P85V5cc19cViiyKAAJQSQkBAFICCIhIQhAQhCJvpbXq1TMa8PF2ms2Ot81fWLinWHber04FR6gGaTZ5/oGaGTat1rFsYBWs9yqglqQ7t6YfaOtOIJGBAFICCIhIQhAQhAJCUEAEoJISAgC6POy+Kuu+k3u/JPinQG//e09YfuXX3650DY19XrYNhcHyO/2Xi7lPH/+YtasWRPaoy0A/upzD7bVh0YEAUgIIiEhCEBCEAkJQQASgkhICALocxxhdHSUDRs2FNr/4s+LbQBHjxXvFh+V3gM48uqiC7Eu8MILL4T206dPh/ZNmzaF9ptvvrnQNj4+HrZdu3ZtaI9iJFGuwkI0IghAQhAJCUEAEoJISAgCkBBEQkIQQBtxhKC83oPAp4GT6dAd7v5s5mwYxTUQRkfjnIFr115faGs2m2Hb6659b2jfsmVLaM/lM+Tsw8PFNRDaqFFRyt4O7UQb5svrfd/MxoGXzOy5ZPuyu3+htBdi4LRTKOMYMF89bdrM5svribcRZcrrAdxlZj8ys11mtrygzXYz22dm+6ampko5K3pH20K4uLwe8CiwDtjM3IjxxcXaLayqNjEx0QWXRS/ouLyeux9396a7t4CvAzf1zk3RazourzdfYzHxceBA990T/aKdWcMW4A+B/Wb2g/TcDuAOM9vMXJ73IaA4Tz1RqVQYGSmeRjU8s9NaLSi/lylB18psB9+y+KUYqsRL9iuZqrLNYCe33OwvN72sVMqHg8qU18vEDMTlhCKLApAQREJCEICEIBISggAkBJHo+27xUcn5U6dOhe2jtO96PS5vl0vrNuI4QzMz2a8MDYX2mUZxuv3YULwDXe4y88wbM4U2z8RP5tGIIAAJQSQkBAFICCIhIQhAQhAJCUEAfS7Tb2YngYXbqr8biIMHg2Op+napfl3r7itzB/VVCG/p3Gyfu984MAcClqpvvfJLXw0CkBBEYtBC2Dng/iOWqm898WugvxHE0mHQI4JYIgxECGZ2q5n9zMxeMbN7BuFDEWZ2yMz2m9kPzGzfgH3ZZWYnzOzAgudWmNlzZnYw3S661PBS6bsQzKwKPAJ8DNjI3PqIjf32I8NH3H3zEpg+fgO49aLn7gEm3X09MJkel2YQI8JNwCvu/gt3rwNPAVsH4MeSx92/B7x20dNbgd3p/m7g9m70NQghXA28uuDxEZbWMnsHvmtmL5nZ9kE7swirU6mC+ZIFq7px0r6mqiUWy7taSlOXLe5+1MxWAc+Z2U/TJ/NtzSBGhCPANQserwGODsCPRXH3o+n2BPA0S2+V9/H5BcjpNq4t3CaDEMKLwHozu97MhoFPAvFmTn3CzK5I5YEwsyuAj7L0VnnvAbal+9uAZ7pyVnfv+x/we8DPgf8B7h2EDwV+/Rbww/T340H7BjzJXBGSWeZG0k8B72JutnAw3a7oRl+KLApAkUWRkBAEICGIhIQgAAlBJCQEAUgIIiEhCAD+Hyw9haAs/+eMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2 \n",
    "img = Image.open('test_license.jpg')\n",
    "image = img.resize((120,50))\n",
    "image = np.array(image)\n",
    "a = (image[0:28,18:30])\n",
    "b, hog_image = hog(image_np,orientations=8, pixels_per_cell=(2, 2), cells_per_block=(1, 1), \n",
    "                                    visualise = True,multichannel=True,feature_vector = True) \n",
    "b = np.array(b,ndmin=2)\n",
    "plt.imshow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-80a2803756d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m __all__ += [name for name in dir(_C)\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-e83f1f95b46b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m __all__ += [name for name in dir(_C)\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "# Why I use pytorch? Because it is much more easier to debug if we compare with tensorflow (Good job M.Z!)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(672, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    # Look again about data, find more about torch data\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # torch.optim as optim on above\n",
    "        optimizer.zero_grad()\n",
    "        # putdata into model\n",
    "        output = model(data)\n",
    "        # loss function of course\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # and we take backprop from loss function (more clearly than tensorflow ... or keras)\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "        # should better use entropy loss, keyword : torch.nn.CrossEntropyLoss\n",
    "\n",
    "net = Net()    \n",
    "params = list(net.parameters())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
