{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from skimage.feature import hog\n",
    "from PIL import Image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "import keras\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OCR_scan(img_path = '', pre = 'thresh', save_path = '', number_pics = ''):\n",
    "    image = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    if pre == 'thresh':\n",
    "        ret, gray = cv2.threshold(gray, 0, 255,cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "    elif pre =='blur':\n",
    "        gray = cv2.medianBlur(gray, 3)\n",
    "    save_into = save_path+number_pics+'.jpg'\n",
    "    cv2.imwrite(save_into, gray) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = 'crop_image\\\\'\n",
    "pre1 = 'thresh'\n",
    "pre2 = 'blur'\n",
    "save_path1 = 'preprocess_thresh\\\\'\n",
    "save_path2 = 'preprocess_blur\\\\'\n",
    "count = '1'\n",
    "count_str = 1\n",
    "for i in os.listdir(img_list):\n",
    "    img_path = img_list+i\n",
    "    OCR_scan(img_path = img_path, pre = pre1, save_path = save_path1, number_pics = count)\n",
    "    OCR_scan(img_path = img_path, pre = pre2, save_path = save_path2, number_pics = count)\n",
    "    count_str += 1\n",
    "    count = str(count_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'preprocess_thresh\\\\'\n",
    "list_thresh = []\n",
    "list_blur = []\n",
    "list_o = []\n",
    "list_test = []\n",
    "for i in range(28):\n",
    "    name = i+1\n",
    "    thresh = filename + str(name) + '.jpg'\n",
    "    blur = 'preprocess_blur\\\\'+ str(name) + '.jpg'\n",
    "    original = 'crop_image\\\\'+ str(name) + '.jpg'\n",
    "    test = 'preprocess_test\\\\'+ str(name) + '.jpg'\n",
    "    list_thresh.append(pytesseract.image_to_string(Image.open(thresh)))\n",
    "    list_blur.append(pytesseract.image_to_string(Image.open(blur)))\n",
    "    list_o.append(pytesseract.image_to_string(Image.open(original)))\n",
    "    list_test.append(pytesseract.image_to_string(Image.open(test)))\n",
    "\n",
    "# print(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#J 59 PA j\n",
      "664.80\n",
      "#62 HS\n",
      "7108\n",
      "#\n",
      "#\n",
      "#1\n",
      "99.97\n",
      "#\n",
      "#ead\n",
      "043.09\n",
      "#\n",
      "#BY 59-41\n",
      "so7 00\n",
      "#59-81\n",
      "385.59\n",
      "#54-H}\n",
      "6982\n",
      "#\n",
      "#59°51\n",
      "09841\n",
      "#81-61\n",
      "135.56\n",
      "#60-F1\n",
      "506.70\n",
      "#Be \"1\n",
      "0200\n",
      "#16M\n",
      "06.\n",
      "#\n",
      "#S5-Po i\n",
      "2840\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#136.29\n",
      "#\n",
      "#59-(2\n",
      "316.61\n"
     ]
    }
   ],
   "source": [
    "for i in list_thresh:\n",
    "    print('#{}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#59897\n",
      "#B 99°C2\n",
      "012.08\n",
      "#ie\n",
      "043.09\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#39-51\n",
      "098.41\n",
      "#81-61\n",
      "135.56\n",
      "#60:F1\n",
      "506.70\n",
      "#\n",
      "#1\n",
      "0\n",
      "\n",
      "6-H\n",
      "61\n",
      "\n",
      "i\n",
      "#\n",
      "#Gar) ¢\n",
      "7840)\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#136.29\n",
      "#70-01\n",
      "/39 16\n",
      "#39-(2\n",
      "316.01\n"
     ]
    }
   ],
   "source": [
    "for i in list_blur:\n",
    "    print('#{}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#, 59-P1 '\n",
      "664.80\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#136.29\n",
      "#\n",
      "#59-(7\n",
      "316.01\n",
      "#\n",
      "#\n",
      "#\n",
      "#U1]\n",
      "98.97\n",
      "#\n",
      "#78-G)\n",
      "043.08)\n",
      "#59-E]\n",
      "708.50\n",
      "#\n",
      "#\n",
      "#36-HT\n",
      "4882\n",
      "#am 59-51\n",
      "098.41)\n",
      "#81-61\n",
      "1135.56\n",
      "#60-F1\n",
      "506.70\n",
      "#63-/\n",
      "0200\n",
      "#\n",
      "#\n",
      "#55-79\n",
      "7840)\n",
      "#\n",
      "#\n"
     ]
    }
   ],
   "source": [
    "for i in list_o:\n",
    "    print('#{}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "unrecognized image mode",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-a657ba8f0905>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# ret, gray = cv2.threshold(closing, 180, 255,cv2.THRESH_BINARY)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'uint8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'gray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpytesseract\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_to_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   2534\u001b[0m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2536\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfrombuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"raw\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mfrombuffer\u001b[1;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[0;32m   2477\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2478\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2479\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfrombytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mfrombytes\u001b[1;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[0;32m   2409\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2411\u001b[1;33m     \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2412\u001b[0m     \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrombytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2413\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mnew\u001b[1;34m(mode, size, color)\u001b[0m\n\u001b[0;32m   2373\u001b[0m         \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageColor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcolor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2375\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: unrecognized image mode"
     ]
    }
   ],
   "source": [
    "img_path = 'crop_image\\\\1.jpg'\n",
    "image = cv2.imread(img_path,0)\n",
    "# plt.imshow(image, cmap = 'gray')\n",
    "# hist,bins = np.histogram(image.flatten(),256,[0,255]) \n",
    "# plt.imshow(hist,cmap = 'gray')\n",
    "img = cv2.equalizeHist(image)\n",
    "ret, img = cv2.threshold(img, 180, 255,cv2.THRESH_BINARY)\n",
    "\n",
    "kernel = np.ones((3,3),np.uint8)\n",
    "closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)\n",
    "# ret, gray = cv2.threshold(closing, 180, 255,cv2.THRESH_BINARY)\n",
    "test = Image.fromarray(closing.astype('uint8'), 'gray')\n",
    "text = pytesseract.image_to_string(Image.open(closing))\n",
    "print(text)\n",
    "plt.imshow(closing, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = 'crop_image\\\\'\n",
    "# img = cv2.imread(img_path,0)\n",
    "# ret, gray = cv2.threshold(img, 150, 255,cv2.THRESH_BINARY)\n",
    "# plt.imshow(gray, cmap = 'gray')\n",
    "def OCR_test(img_path = '', pre = 'thresh', save_path = '', number_pics = ''):\n",
    "    image = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    if pre == 'thresh':\n",
    "        ret, gray = cv2.threshold(gray, 180, 255,cv2.THRESH_BINARY)\n",
    "#     elif pre =='blur':\n",
    "#         gray = cv2.medianBlur(gray, 3)\n",
    "    save_into = save_path+number_pics+'.jpg'\n",
    "    cv2.imwrite(save_into, gray) \n",
    "count = '1'\n",
    "count_str = 1\n",
    "for i in os.listdir(img_list):\n",
    "    img_path = img_list+i\n",
    "    OCR_test(img_path = img_path, pre = 'thresh', save_path = 'preprocess_test\\\\', number_pics = count)\n",
    "    count_str += 1\n",
    "    count = str(count_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#59-P1\n",
      "661.80\n",
      "#\n",
      "#FT,\n",
      "170.78\n",
      "#\n",
      "#59-1\n",
      "98.97]\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#59-B1 |\n",
      "38559\n",
      "#50H\n",
      "6882 fi\n",
      "#\n",
      "#9-51 |\n",
      "098.41]\n",
      "#\n",
      "#§0-F\n",
      "506.70)\n",
      "#63-41]\n",
      "02004\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#ae “TT\n",
      "#\n",
      "#\n",
      "#\n",
      "#59-ul\n",
      "136.29\n",
      "#1-C1\n",
      "789.15\n",
      "#\n"
     ]
    }
   ],
   "source": [
    "list_test = []\n",
    "for i in range(28):\n",
    "    name = i+1\n",
    "    test = 'preprocess_test\\\\'+ str(name) + '.jpg'\n",
    "    list_test.append(pytesseract.image_to_string(Image.open(test)))\n",
    "for i in list_test:\n",
    "    print('#{}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Using real-time data augmentation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`steps_per_epoch=None` is only valid for a generator based on the `keras.utils.Sequence` class. Please specify `steps_per_epoch` or use the `keras.utils.Sequence` class.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8a3414cf39f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    117\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m                         workers=4)\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;31m# Save model and weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             raise ValueError('`steps_per_epoch=None` is only valid for a'\n\u001b[0m\u001b[0;32m     56\u001b[0m                              \u001b[1;34m' generator based on the '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m                              \u001b[1;34m'`keras.utils.Sequence`'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: `steps_per_epoch=None` is only valid for a generator based on the `keras.utils.Sequence` class. Please specify `steps_per_epoch` or use the `keras.utils.Sequence` class."
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Train a simple deep CNN on the CIFAR10 small images dataset.\n",
    "\n",
    "It gets to 75% validation accuracy in 25 epochs, and 79% after 50 epochs.\n",
    "(it's still underfitting at that point, though).\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "img = Image.open('five.jpg')\n",
    "# print(np.array(img).shape)\n",
    "image = img.resize((28,28), Image.LANCZOS)\n",
    "image = np.array(image)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[[1,2,3],[1,2,3]],[[1,2,3],[1,2,3]]])\n",
    "b = np.array([[[1,2,3],[1,2,3]],[[1,2,3],[1,2,3]],[[1,2,3],[1,2,3]]])\n",
    "\n",
    "print(np.concatenate((a,b),axis=0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 9.293680297397769 %\n",
      "Done 18.587360594795538 %\n",
      "Done 27.881040892193308 %\n",
      "Done 37.174721189591075 %\n",
      "Done 46.468401486988846 %\n",
      "Done 55.762081784386616 %\n",
      "Done 65.05576208178438 %\n",
      "Done 74.34944237918215 %\n",
      "Done 83.64312267657992 %\n",
      "Done 92.93680297397769 %\n",
      "Done 100 %\n"
     ]
    }
   ],
   "source": [
    "# Take hog vetor and set label for all data positive\n",
    "positive_path = '.\\\\charTrainset\\\\'\n",
    "X_train_positive = np.zeros((1076,672))\n",
    "label_positive = []\n",
    "count = 0\n",
    "for digits in os.listdir(positive_path):\n",
    "    for pics in os.listdir(positive_path+digits):\n",
    "        image = cv2.imread(positive_path+digits+'\\\\'+pics)\n",
    "        X_train_positive[count], hog_image = hog(image,orientations=8, pixels_per_cell=(2, 2), cells_per_block=(1, 1), \n",
    "                                        visualise = True,multichannel=True,feature_vector = True)\n",
    "        label_positive.append(1)\n",
    "        count+=1\n",
    "        if count%100==0:\n",
    "            print('Done {} %'.format((count*100)/1076))\n",
    "print('Done 100 %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 1.075268817204301 %\n",
      "Done 2.150537634408602 %\n",
      "Done 3.225806451612903 %\n",
      "Done 4.301075268817204 %\n",
      "Done 5.376344086021505 %\n",
      "Done 6.451612903225806 %\n",
      "Done 7.526881720430108 %\n",
      "Done 8.602150537634408 %\n",
      "Done 9.67741935483871 %\n",
      "Done 10.75268817204301 %\n",
      "Done 11.827956989247312 %\n",
      "Done 12.903225806451612 %\n",
      "Done 13.978494623655914 %\n",
      "Done 15.053763440860216 %\n",
      "Done 16.129032258064516 %\n",
      "Done 17.204301075268816 %\n",
      "Done 18.27956989247312 %\n",
      "Done 19.35483870967742 %\n",
      "Done 20.43010752688172 %\n",
      "Done 21.50537634408602 %\n",
      "Done 22.580645161290324 %\n",
      "Done 23.655913978494624 %\n",
      "Done 24.731182795698924 %\n",
      "Done 25.806451612903224 %\n",
      "Done 26.881720430107528 %\n",
      "Done 27.956989247311828 %\n",
      "Done 29.032258064516128 %\n",
      "Done 30.107526881720432 %\n",
      "Done 31.182795698924732 %\n",
      "Done 32.25806451612903 %\n",
      "Done 33.333333333333336 %\n",
      "Done 34.40860215053763 %\n",
      "Done 35.483870967741936 %\n",
      "Done 36.55913978494624 %\n",
      "Done 37.634408602150536 %\n",
      "Done 38.70967741935484 %\n",
      "Done 39.784946236559136 %\n",
      "Done 40.86021505376344 %\n",
      "Done 41.935483870967744 %\n",
      "Done 43.01075268817204 %\n",
      "Done 44.086021505376344 %\n",
      "Done 45.16129032258065 %\n",
      "Done 46.236559139784944 %\n",
      "Done 47.31182795698925 %\n",
      "Done 48.38709677419355 %\n",
      "Done 49.46236559139785 %\n",
      "Done 50.53763440860215 %\n",
      "Done 51.61290322580645 %\n",
      "Done 52.68817204301075 %\n",
      "Done 53.763440860215056 %\n",
      "Done 54.83870967741935 %\n",
      "Done 55.913978494623656 %\n",
      "Done 56.98924731182796 %\n",
      "Done 58.064516129032256 %\n",
      "Done 59.13978494623656 %\n",
      "Done 60.215053763440864 %\n",
      "Done 61.29032258064516 %\n",
      "Done 62.365591397849464 %\n",
      "Done 63.44086021505376 %\n",
      "Done 64.51612903225806 %\n",
      "Done 65.59139784946237 %\n",
      "Done 66.66666666666667 %\n",
      "Done 67.74193548387096 %\n",
      "Done 68.81720430107526 %\n",
      "Done 69.89247311827957 %\n",
      "Done 70.96774193548387 %\n",
      "Done 72.04301075268818 %\n",
      "Done 73.11827956989248 %\n",
      "Done 74.19354838709677 %\n",
      "Done 75.26881720430107 %\n",
      "Done 76.34408602150538 %\n",
      "Done 77.41935483870968 %\n",
      "Done 78.49462365591398 %\n",
      "Done 79.56989247311827 %\n",
      "Done 80.64516129032258 %\n",
      "Done 81.72043010752688 %\n",
      "Done 82.79569892473118 %\n",
      "Done 83.87096774193549 %\n",
      "Done 84.94623655913979 %\n",
      "Done 86.02150537634408 %\n",
      "Done 87.09677419354838 %\n",
      "Done 88.17204301075269 %\n",
      "Done 89.24731182795699 %\n",
      "Done 90.3225806451613 %\n",
      "Done 91.39784946236558 %\n",
      "Done 92.47311827956989 %\n",
      "Done 93.54838709677419 %\n",
      "Done 94.6236559139785 %\n",
      "Done 95.6989247311828 %\n",
      "Done 96.7741935483871 %\n",
      "Done 97.84946236559139 %\n",
      "Done 98.9247311827957 %\n",
      "Done 100.0 %\n",
      "Done 100 %\n"
     ]
    }
   ],
   "source": [
    "# Take hog vetor and set label for all data negative\n",
    "negative_path1 = ['.\\\\training-samples\\\\detection\\\\negatives\\\\','.\\\\training-samples\\\\icdar\\\\negatives\\\\']\n",
    "X_train_negative = np.zeros((9300,672))\n",
    "label_negative = []\n",
    "count = 0\n",
    "for path in negative_path1:\n",
    "    for pics in os.listdir(path):\n",
    "        img = Image.open(path+pics)\n",
    "        image = img.resize((12,28))\n",
    "        image_np = np.array(image)\n",
    "        X_train_negative[count], hog_image = hog(image_np,orientations=8, pixels_per_cell=(2, 2), cells_per_block=(1, 1), \n",
    "                                        visualise = True,multichannel=True,feature_vector = True)\n",
    "        label_negative.append(0)\n",
    "        count+=1\n",
    "        if count%100==0:\n",
    "            print('Done {} %'.format((count*100)/9300))\n",
    "print('Done 100 %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine together\n",
    "X_train = np.concatenate((X_train_positive,X_train_negative), axis = 0)\n",
    "y_train = label_positive + label_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 88.49557522123894 %\n",
      "Done 100 %\n"
     ]
    }
   ],
   "source": [
    "# Take hog vetor and set label for all test data negative\n",
    "test_path = '.\\\\test_data\\\\negative\\\\'\n",
    "X_test_negative = np.zeros((113,672))\n",
    "label_test_negative = []\n",
    "count = 0\n",
    "for pics in os.listdir(test_path):\n",
    "    img = Image.open(test_path+pics)\n",
    "    image = img.resize((12,28))\n",
    "    image_np = np.array(image)\n",
    "    X_test_negative[count], hog_image = hog(image_np,orientations=8, pixels_per_cell=(2, 2), cells_per_block=(1, 1), \n",
    "                                    visualise = True,multichannel=True,feature_vector = True)\n",
    "    label_test_negative.append(0)\n",
    "    count+=1\n",
    "    if count%100==0:\n",
    "        print('Done {} %'.format((count*100)/113))\n",
    "print('Done 100 %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take hog vetor and set label for all test data positive\n",
    "test_path = 'C:\\\\Users\\\\ADMINS\\\\Desktop\\\\train_32x32.mat'\n",
    "dict_mat = scipy.io.loadmat(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "all_pics = dict_mat['X']\n",
    "shape = all_pics.shape[3]\n",
    "X_test_positive = np.zeros((100,672))\n",
    "label_test_positive = []\n",
    "for count in range(100):\n",
    "    img = all_pics[:,:,:,count]\n",
    "    img = Image.fromarray(img)\n",
    "    image = img.resize((12,28))\n",
    "    image_np = np.array(image)\n",
    "    X_test_positive[count], hog_image = hog(image_np,orientations=8, pixels_per_cell=(2, 2), cells_per_block=(1, 1), \n",
    "                                    visualise = True,multichannel=True,feature_vector = True)\n",
    "    label_test_positive.append(1)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine together and then save to pickle file\n",
    "X_test = np.concatenate((X_test_positive,X_test_negative), axis = 0)\n",
    "y_test = label_test_positive + label_test_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save all data of hog vector into mat file\n",
    "data_dict = {'X_train':X_train,'y_train':y_train,'X_test':X_test,'y_test':y_test}\n",
    "scipy.io.savemat('dataTraining',data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10376, 672) (1, 10376) (213, 672) (1, 213)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#load data from mat file\n",
    "data_path = '.\\\\dataTraining.mat'\n",
    "data = scipy.io.loadmat(data_path)\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "X_test  = data['X_test']\n",
    "y_test  = data['y_test']\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10376, 2)\n",
      "Epoch 1/10\n",
      "10376/10376 [==============================] - 0s 39us/step - loss: 0.6985 - acc: 0.8871\n",
      "Epoch 2/10\n",
      "10376/10376 [==============================] - 0s 6us/step - loss: 0.5072 - acc: 0.8963\n",
      "Epoch 3/10\n",
      "10376/10376 [==============================] - 0s 5us/step - loss: 0.5006 - acc: 0.8963\n",
      "Epoch 4/10\n",
      "10376/10376 [==============================] - 0s 6us/step - loss: 0.5006 - acc: 0.8963\n",
      "Epoch 5/10\n",
      "10376/10376 [==============================] - 0s 5us/step - loss: 0.5005 - acc: 0.8963\n",
      "Epoch 6/10\n",
      "10376/10376 [==============================] - 0s 5us/step - loss: 0.5005 - acc: 0.8963\n",
      "Epoch 7/10\n",
      "10376/10376 [==============================] - 0s 5us/step - loss: 0.5005 - acc: 0.8963\n",
      "Epoch 8/10\n",
      "10376/10376 [==============================] - 0s 5us/step - loss: 0.5005 - acc: 0.8963\n",
      "Epoch 9/10\n",
      "10376/10376 [==============================] - 0s 6us/step - loss: 0.5004 - acc: 0.8963\n",
      "Epoch 10/10\n",
      "10376/10376 [==============================] - 0s 5us/step - loss: 0.5004 - acc: 0.8963\n",
      "Test loss: 0.5\n",
      "Test accuracy: 0.5305164319248826\n"
     ]
    }
   ],
   "source": [
    "# Training window for recognize digits in license plate\n",
    "batch_size = 1860\n",
    "num_classes = 2\n",
    "epochs = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "print(y_train.shape)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "model = Sequential()\n",
    "model.add(Dense(60, activation='relu'))\n",
    "model.add(Dense(150, activation='relu'))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(num_classes, activation = 'relu'))\n",
    "model.compile(loss='hinge',\n",
    "              optimizer=keras.optimizers.Adam(lr=0.001,beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1)\n",
    "#           validation_data=(X_test, y_test))\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x27ac0ba72e8>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2 \n",
    "img = Image.open('test_license.jpg')\n",
    "image = img.resize((120,50))\n",
    "image = np.array(image)\n",
    "a = (image[0:28,18:30])\n",
    "b, hog_image = hog(image_np,orientations=8, pixels_per_cell=(2, 2), cells_per_block=(1, 1), \n",
    "                                    visualise = True,multichannel=True,feature_vector = True) \n",
    "b = np.array(b,ndmin=2)\n",
    "plt.imshow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
